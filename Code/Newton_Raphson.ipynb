{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Newton_Raphson.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxNSuVuFkKgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a44b8487-ea43-4d62-992d-73a185d75b11"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXj5PsCgcesM",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we have attempted to build a neural network capable of solving quadratic equations with a single variable x. The expected speed up from the Neural network will make it superior to the traditional Newton Raphson method used to solve such equations.\n",
        "\n",
        "The neural network solves an equation of the form\n",
        "\n",
        "a*(x^2) + b*(x) + c = 0 \n",
        "\n",
        "and predicts as output the value of x (the solution to the equation)\n",
        "\n",
        "\n",
        "Thus, the neural network has 3 inputs in the first layer and one output cell in the output layer. The architecture of the neural network is as mentioned the research paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFK6wndCefSk",
        "colab_type": "text"
      },
      "source": [
        "We have made use of keras and tensorflow to create the neural network, while numpy was used to generate the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLBymL2tDLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import savetxt"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCjrcuE6VH_8",
        "colab_type": "text"
      },
      "source": [
        "Generating the Data: \n",
        "\n",
        "Values of a,b,c each in the range of -20 to 20 are generated and the corresponding value of the roots of x are found using numpy.\n",
        "\n",
        "The values of a,b,c are normalised such that they lie between -1 and 1. Only those values of a,b,c are considered which produce a real \n",
        "value of x between -1 and 1. This is done to prevent the error at each step of training from getting too large.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxFUFR40vnO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b458c12-824a-4a9b-ada6-2f14a8bbd530"
      },
      "source": [
        "cnt = 0\n",
        "target = np.zeros((1000000, 1))\n",
        "input = np.zeros((1000000, 3))\n",
        "\n",
        "\n",
        "\n",
        "for i in range(-20, 20):\n",
        "  if i == 0:\n",
        "    continue\n",
        "  for j in range(-20, 20):\n",
        "    for k in range(-20, 20):\n",
        "      \n",
        "      coeff = [i /20, j / 20, k / 20]\n",
        "      roots = np.roots(coeff)\n",
        "\n",
        "      if all(isinstance(l, float) for l in roots) and abs(np.max(roots)) <= 1:\n",
        "\n",
        "        # for i in range(3):\n",
        "        #   print(coeff[i], end = ' ')\n",
        "        # print(np.max(roots))\n",
        "\n",
        "        target[cnt] = np.max(roots)\n",
        "        input[cnt] = coeff\n",
        "        cnt += 1\n",
        "\n",
        "print(\"Total number of examples =\", cnt)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of examples = 20292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-LmufwLy8-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = target[ :cnt, :] \n",
        "X = input[:cnt, :]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV8nnC4HewJc",
        "colab_type": "text"
      },
      "source": [
        "Now, we split the data into train and test sets and store the data in csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFk5M5crWoP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)\n",
        "\n",
        "savetxt('X_train.csv', X_train, delimiter = ',')\n",
        "savetxt('X_test.csv', X_test, delimiter = ',')\n",
        "savetxt('Y_train.csv', Y_train, delimiter = ',')\n",
        "savetxt('Y_test.csv', Y_test, delimiter = ',')\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi22uGZfe96-",
        "colab_type": "text"
      },
      "source": [
        "The first neural network we created has a 3x5x3x1 four layered architecture. The activations functions we selected were the ones that gave us the best results on the training data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrXYzAbKWAQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_1 = keras.models.Sequential([\n",
        "    keras.layers.Dense(3, activation = 'linear'),\n",
        "    keras.layers.Dense(5, activation = 'relu'), \n",
        "    keras.layers.Dense(3, activation = 'relu'),\n",
        "    keras.layers.Dense(1, activation = 'linear')\n",
        "])\n",
        "\n",
        "model_1.compile(loss='mean_squared_error', optimizer = 'adam', metrics = [\"accuracy\"])\n",
        "\n",
        "model_1.build()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "795CCgZYdEcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61354d82-a67c-4d90-ccb9-30bb726b5484"
      },
      "source": [
        "H1 = model_1.fit(X_train, Y_train, epochs = 100)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16233/16233 [==============================] - 2s 129us/step - loss: 0.1389 - accuracy: 0.0540\n",
            "Epoch 2/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0732 - accuracy: 0.0675\n",
            "Epoch 3/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0440 - accuracy: 0.0760\n",
            "Epoch 4/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0306 - accuracy: 0.0817\n",
            "Epoch 5/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0234 - accuracy: 0.0837\n",
            "Epoch 6/100\n",
            "16233/16233 [==============================] - 2s 119us/step - loss: 0.0202 - accuracy: 0.0841\n",
            "Epoch 7/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0184 - accuracy: 0.0842\n",
            "Epoch 8/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0173 - accuracy: 0.0861\n",
            "Epoch 9/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0166 - accuracy: 0.0869\n",
            "Epoch 10/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0161 - accuracy: 0.0870\n",
            "Epoch 11/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0157 - accuracy: 0.0869\n",
            "Epoch 12/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0154 - accuracy: 0.0870\n",
            "Epoch 13/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0151 - accuracy: 0.0870\n",
            "Epoch 14/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0150 - accuracy: 0.0872\n",
            "Epoch 15/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0148 - accuracy: 0.0872\n",
            "Epoch 16/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0147 - accuracy: 0.0874\n",
            "Epoch 17/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0146 - accuracy: 0.0872\n",
            "Epoch 18/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0145 - accuracy: 0.0872\n",
            "Epoch 19/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0144 - accuracy: 0.0871\n",
            "Epoch 20/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0143 - accuracy: 0.0870\n",
            "Epoch 21/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0144 - accuracy: 0.0875\n",
            "Epoch 22/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0143 - accuracy: 0.0873\n",
            "Epoch 23/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0141 - accuracy: 0.0873\n",
            "Epoch 24/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0141 - accuracy: 0.0874\n",
            "Epoch 25/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0142 - accuracy: 0.0875\n",
            "Epoch 26/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0141 - accuracy: 0.0874\n",
            "Epoch 27/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0141 - accuracy: 0.0873\n",
            "Epoch 28/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0140 - accuracy: 0.0875\n",
            "Epoch 29/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0141 - accuracy: 0.0875\n",
            "Epoch 30/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0140 - accuracy: 0.0874\n",
            "Epoch 31/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0141 - accuracy: 0.0875\n",
            "Epoch 32/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0140 - accuracy: 0.0874\n",
            "Epoch 33/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0139 - accuracy: 0.0875\n",
            "Epoch 34/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0140 - accuracy: 0.0875\n",
            "Epoch 35/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0139 - accuracy: 0.0875\n",
            "Epoch 36/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0139 - accuracy: 0.0877\n",
            "Epoch 37/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0138 - accuracy: 0.0875\n",
            "Epoch 38/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0138 - accuracy: 0.0876\n",
            "Epoch 39/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0138 - accuracy: 0.0876\n",
            "Epoch 40/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0138 - accuracy: 0.0877\n",
            "Epoch 41/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0138 - accuracy: 0.0876\n",
            "Epoch 42/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0138 - accuracy: 0.0877\n",
            "Epoch 43/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0138 - accuracy: 0.0877\n",
            "Epoch 44/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0138 - accuracy: 0.0875\n",
            "Epoch 45/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0138 - accuracy: 0.0877\n",
            "Epoch 46/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0138 - accuracy: 0.0875\n",
            "Epoch 47/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 48/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0138 - accuracy: 0.0876\n",
            "Epoch 49/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 50/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0137 - accuracy: 0.0876\n",
            "Epoch 51/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0136 - accuracy: 0.0876\n",
            "Epoch 52/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 53/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 54/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 55/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0136 - accuracy: 0.0876\n",
            "Epoch 56/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 57/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 58/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0136 - accuracy: 0.0875\n",
            "Epoch 59/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0136 - accuracy: 0.0878\n",
            "Epoch 60/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 61/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 62/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 63/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0137 - accuracy: 0.0877\n",
            "Epoch 64/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 65/100\n",
            "16233/16233 [==============================] - 2s 126us/step - loss: 0.0134 - accuracy: 0.0877\n",
            "Epoch 66/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 67/100\n",
            "16233/16233 [==============================] - 2s 128us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 68/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 69/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 70/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0875\n",
            "Epoch 71/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0135 - accuracy: 0.0880\n",
            "Epoch 72/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0136 - accuracy: 0.0876\n",
            "Epoch 73/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 74/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 75/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0134 - accuracy: 0.0878\n",
            "Epoch 76/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 77/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 78/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 79/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0134 - accuracy: 0.0879\n",
            "Epoch 80/100\n",
            "16233/16233 [==============================] - 2s 119us/step - loss: 0.0134 - accuracy: 0.0877\n",
            "Epoch 81/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0134 - accuracy: 0.0875\n",
            "Epoch 82/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 83/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 84/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0134 - accuracy: 0.0879\n",
            "Epoch 85/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0134 - accuracy: 0.0877\n",
            "Epoch 86/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 87/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0134 - accuracy: 0.0878\n",
            "Epoch 88/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 89/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 90/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 91/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 92/100\n",
            "16233/16233 [==============================] - 2s 120us/step - loss: 0.0135 - accuracy: 0.0878\n",
            "Epoch 93/100\n",
            "16233/16233 [==============================] - 2s 123us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 94/100\n",
            "16233/16233 [==============================] - 2s 124us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 95/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0134 - accuracy: 0.0877\n",
            "Epoch 96/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0134 - accuracy: 0.0880\n",
            "Epoch 97/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0134 - accuracy: 0.0877\n",
            "Epoch 98/100\n",
            "16233/16233 [==============================] - 2s 125us/step - loss: 0.0136 - accuracy: 0.0877\n",
            "Epoch 99/100\n",
            "16233/16233 [==============================] - 2s 122us/step - loss: 0.0135 - accuracy: 0.0877\n",
            "Epoch 100/100\n",
            "16233/16233 [==============================] - 2s 121us/step - loss: 0.0135 - accuracy: 0.0877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7levcQfUxZ",
        "colab_type": "text"
      },
      "source": [
        "When we fit the network to training data, we observe that loss decreases sharply in the beginning, and then the decrease is very minimal in later epochs. As for the accuracy, it is very low to start with, and only marginally increases throughout the iterations.\n",
        "\n",
        "When we evaluate our model on the test set, we get similar results to that of the training set. The loss is slightly higher and the accuracy is more or less the same.\n",
        "\n",
        "In either case, the highest accuracy we achived (0.10) was nowhere close to the 93% reported in the paper. This was despite trying out several combinations of activation layer functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOaPKD6HFT8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "16a7e5ce-9d2b-4f7a-c9b4-509554d5db7f"
      },
      "source": [
        "model_1.evaluate(X_test, Y_test, verbose = 1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4059/4059 [==============================] - 0s 53us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.013663310667852788, 0.09189455211162567]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbsrWJPTf982",
        "colab_type": "text"
      },
      "source": [
        "The second neural network we created has a 3 x 11 x 8 x 5 x 1 five layered architecture. The activations functions we selected were the ones that gave us the best results on the training data. We expected a deeper network to yield higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-GEMLF4YEL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2 = keras.models.Sequential([\n",
        "    keras.layers.Dense(3, activation = 'linear'),\n",
        "    keras.layers.Dense(11, activation = 'relu'), \n",
        "    keras.layers.Dense(8, activation = 'relu'),\n",
        "    keras.layers.Dense(5, activation = 'relu'),\n",
        "    keras.layers.Dense(1, activation = 'linear')\n",
        "])\n",
        "\n",
        "model_2.compile(loss='mean_squared_error', optimizer = 'adam', metrics = [\"accuracy\"])\n",
        "\n",
        "model_2.build()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q9prT7WZSqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ac756bd-239e-4f1a-fe9e-e41908a9eefb"
      },
      "source": [
        "H2 = model_2.fit(X_train, Y_train, epochs = 100)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "16233/16233 [==============================] - 2s 142us/step - loss: 0.0880 - accuracy: 0.0691\n",
            "Epoch 2/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0273 - accuracy: 0.0842\n",
            "Epoch 3/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0130 - accuracy: 0.0872\n",
            "Epoch 4/100\n",
            "16233/16233 [==============================] - 2s 132us/step - loss: 0.0085 - accuracy: 0.0879\n",
            "Epoch 5/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0069 - accuracy: 0.0881\n",
            "Epoch 6/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0063 - accuracy: 0.0884\n",
            "Epoch 7/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0060 - accuracy: 0.0886\n",
            "Epoch 8/100\n",
            "16233/16233 [==============================] - 2s 146us/step - loss: 0.0057 - accuracy: 0.0885\n",
            "Epoch 9/100\n",
            "16233/16233 [==============================] - 2s 145us/step - loss: 0.0055 - accuracy: 0.0886\n",
            "Epoch 10/100\n",
            "16233/16233 [==============================] - 2s 145us/step - loss: 0.0052 - accuracy: 0.0886\n",
            "Epoch 11/100\n",
            "16233/16233 [==============================] - 2s 149us/step - loss: 0.0048 - accuracy: 0.0891\n",
            "Epoch 12/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0045 - accuracy: 0.0892\n",
            "Epoch 13/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0042 - accuracy: 0.0893\n",
            "Epoch 14/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0040 - accuracy: 0.0893\n",
            "Epoch 15/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0039 - accuracy: 0.0894\n",
            "Epoch 16/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0037 - accuracy: 0.0893\n",
            "Epoch 17/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0036 - accuracy: 0.0894\n",
            "Epoch 18/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0035 - accuracy: 0.0895\n",
            "Epoch 19/100\n",
            "16233/16233 [==============================] - 2s 132us/step - loss: 0.0034 - accuracy: 0.0895\n",
            "Epoch 20/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0033 - accuracy: 0.0897\n",
            "Epoch 21/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0033 - accuracy: 0.0896\n",
            "Epoch 22/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0031 - accuracy: 0.0896\n",
            "Epoch 23/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0030 - accuracy: 0.0897\n",
            "Epoch 24/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0029 - accuracy: 0.0897\n",
            "Epoch 25/100\n",
            "16233/16233 [==============================] - 2s 137us/step - loss: 0.0029 - accuracy: 0.0898\n",
            "Epoch 26/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0028 - accuracy: 0.0898\n",
            "Epoch 27/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0027 - accuracy: 0.0898\n",
            "Epoch 28/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0027 - accuracy: 0.0898\n",
            "Epoch 29/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0026 - accuracy: 0.0898\n",
            "Epoch 30/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0025 - accuracy: 0.0901\n",
            "Epoch 31/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0025 - accuracy: 0.0901\n",
            "Epoch 32/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0024 - accuracy: 0.0901\n",
            "Epoch 33/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0023 - accuracy: 0.0902\n",
            "Epoch 34/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0022 - accuracy: 0.0902\n",
            "Epoch 35/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0021 - accuracy: 0.0902\n",
            "Epoch 36/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0021 - accuracy: 0.0901\n",
            "Epoch 37/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0020 - accuracy: 0.0902\n",
            "Epoch 38/100\n",
            "16233/16233 [==============================] - 2s 137us/step - loss: 0.0019 - accuracy: 0.0902\n",
            "Epoch 39/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0019 - accuracy: 0.0902\n",
            "Epoch 40/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0019 - accuracy: 0.0902\n",
            "Epoch 41/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0018 - accuracy: 0.0902\n",
            "Epoch 42/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0017 - accuracy: 0.0902\n",
            "Epoch 43/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0017 - accuracy: 0.0903\n",
            "Epoch 44/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0017 - accuracy: 0.0903\n",
            "Epoch 45/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0016 - accuracy: 0.0903\n",
            "Epoch 46/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0017 - accuracy: 0.0903\n",
            "Epoch 47/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0016 - accuracy: 0.0903\n",
            "Epoch 48/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0016 - accuracy: 0.0903\n",
            "Epoch 49/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0016 - accuracy: 0.0903\n",
            "Epoch 50/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0016 - accuracy: 0.0903\n",
            "Epoch 51/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0016 - accuracy: 0.0903\n",
            "Epoch 52/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 53/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0015 - accuracy: 0.0904\n",
            "Epoch 54/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 55/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 56/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 57/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 58/100\n",
            "16233/16233 [==============================] - 2s 132us/step - loss: 0.0015 - accuracy: 0.0904\n",
            "Epoch 59/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 60/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 61/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0015 - accuracy: 0.0904\n",
            "Epoch 62/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 63/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0015 - accuracy: 0.0903\n",
            "Epoch 64/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 65/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 66/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 67/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 68/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 69/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 70/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 71/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 72/100\n",
            "16233/16233 [==============================] - 2s 143us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 73/100\n",
            "16233/16233 [==============================] - 2s 144us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 74/100\n",
            "16233/16233 [==============================] - 2s 138us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 75/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 76/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 77/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0014 - accuracy: 0.0904\n",
            "Epoch 78/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 79/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0014 - accuracy: 0.0903\n",
            "Epoch 80/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0013 - accuracy: 0.0903\n",
            "Epoch 81/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0013 - accuracy: 0.0903\n",
            "Epoch 82/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 83/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 84/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 85/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0013 - accuracy: 0.0903\n",
            "Epoch 86/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 87/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0013 - accuracy: 0.0903\n",
            "Epoch 88/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 89/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 90/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 91/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0013 - accuracy: 0.0904\n",
            "Epoch 92/100\n",
            "16233/16233 [==============================] - 2s 136us/step - loss: 0.0013 - accuracy: 0.0903\n",
            "Epoch 93/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0012 - accuracy: 0.0904\n",
            "Epoch 94/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0012 - accuracy: 0.0904\n",
            "Epoch 95/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0012 - accuracy: 0.0903\n",
            "Epoch 96/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0012 - accuracy: 0.0903\n",
            "Epoch 97/100\n",
            "16233/16233 [==============================] - 2s 133us/step - loss: 0.0012 - accuracy: 0.0904\n",
            "Epoch 98/100\n",
            "16233/16233 [==============================] - 2s 134us/step - loss: 0.0012 - accuracy: 0.0904\n",
            "Epoch 99/100\n",
            "16233/16233 [==============================] - 2s 135us/step - loss: 0.0012 - accuracy: 0.0904\n",
            "Epoch 100/100\n",
            "16233/16233 [==============================] - 2s 131us/step - loss: 0.0012 - accuracy: 0.0904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jjovHqRBgkaN"
      },
      "source": [
        "When we fit the second network to the training data, we observe that loss decreases sharply in the beginning, and then the decrease is very minimal in later epochs, similar to the first neural network. As for the accuracy, it is slighlty higher than the previous 4 layered network, but is still extremely low.\n",
        "\n",
        "When we evaluate our model on the test set, we get similar results to that of the training set. The loss and accuracy are identical to that in the training set indicating that the data generalizes well (no overfitting). \n",
        "\n",
        "The highest accuracy we achived (0.11) was far lesser than the 95% reported in the paper. This was despite trying out several combinations of activation layers functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Byr8OUZnip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99b90a79-2c3e-4159-b0bf-73148391a6dd"
      },
      "source": [
        "model_2.evaluate(X_test, Y_test, verbose = 1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4059/4059 [==============================] - 0s 61us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0011711425048456993, 0.09608277678489685]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    }
  ]
}